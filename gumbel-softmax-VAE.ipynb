{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "TEMPERATURE = 1.0\n",
    "TEMP_MIN = 0.5\n",
    "ANNEAL_RATE = 0.00003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "eval_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "eval_loader = torch.utils.data.DataLoader(dataset=eval_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gumbel_Softmax_VAE(nn.Module):\n",
    "    def __init__(self, x_dim, h1, h2, z_dim, c_dim):\n",
    "        super(Gumbel_Softmax_VAE, self).__init__()\n",
    "        \n",
    "        self.z_dim = z_dim\n",
    "        self.c_dim = c_dim\n",
    "        \n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Linear(x_dim, h1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h1, h2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h2, z_dim*c_dim)\n",
    "        )\n",
    "        \n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(z_dim*c_dim, h2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h2, h1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h1, x_dim),\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x, temperature):\n",
    "        q_y = self.enc(x).view(x.size(0), self.z_dim, self.c_dim)\n",
    "        return self.dec(self.gumbel_softmax(q_y, temperature)), F.softmax(q_y, dim=-1).view(x.size(0), -1)\n",
    "    \n",
    "    def sample_gumbel(self, shape, eps=1e-20):\n",
    "        U = torch.rand(shape)\n",
    "        return -torch.log(-torch.log(U + eps) + eps)\n",
    "    \n",
    "    def gumbel_softmax(self, logits, temperature):\n",
    "        y = logits + self.sample_gumbel(logits.size())\n",
    "        return F.softmax( y / temperature, dim=-1).view(y.size(0), -1)\n",
    "        \n",
    "\n",
    "vae = Gumbel_Softmax_VAE(x_dim=784, h1=512, h2=256, z_dim=30, c_dim=10)\n",
    "if torch.cuda.is_available():\n",
    "    vae.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gumbel_Softmax_VAE(\n",
       "  (enc): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=300, bias=True)\n",
       "  )\n",
       "  (dec): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=784, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function \n",
    "\n",
    "ELBO :\n",
    "\n",
    "$ logp_\\theta(x) \\ge E_{q_\\phi(y|x)}[logp_\\theta(x|y)] - KL[q_\\phi(y|x) \\| p_\\theta(y)] $\n",
    "\n",
    "KL : \n",
    "\n",
    "$ KLD = \\sum_{y} q_\\phi(y|x) [ log{\\frac{q_\\phi(y|x)}{p_\\theta(y)}} ]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(recon_x, x, qy):\n",
    "    # reconstruction loss : binary cross entropy \n",
    "    bce_loss = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "    # kl divergence \n",
    "    \n",
    "    log_qy = torch.log(qy+1e-20)\n",
    "    log_py = Variable(torch.log(torch.Tensor([1.0/10])))\n",
    "    kld_loss = torch.sum(qy*(log_qy - log_py),dim=-1).mean()\n",
    "\n",
    "    return bce_loss + kld_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(vae.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    temp = TEMPERATURE\n",
    "    for batch_ind, (data, _) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        data = data.view(BATCH_SIZE, -1)\n",
    "        \n",
    "        if batch_ind % 100 == 1:\n",
    "            temp = np.maximum(temp * np.exp(-ANNEAL_RATE * batch_ind), TEMP_MIN)\n",
    "\n",
    "        recon_x, qy = vae(data, temp)\n",
    "        loss = loss_fn(recon_x, data, qy)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_ind % 200 == 0:\n",
    "            print('Train Epoch:{} [{}/{} ({:0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_ind*len(data), len(train_loader.dataset),\n",
    "                100*batch_ind/len(train_loader), loss.item()/len(data)))\n",
    "        \n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss/len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(): \n",
    "    vae.eval()\n",
    "    eval_loss = 0\n",
    "    temp = TEMPERATURE\n",
    "    with torch.no_grad():\n",
    "        for batch_ind, (data, _) in enumerate(eval_loader):\n",
    "#             data = data.cuda()\n",
    "            if batch_ind % 100 == 1:\n",
    "                temp = np.maximum(temp * np.exp(-ANNEAL_RATE * batch_ind), TEMP_MIN)\n",
    "            data = data.view(BATCH_SIZE, -1)\n",
    "            recon, qy = vae(data, temp)\n",
    "            eval_loss += loss_fn(recon, data, qy)\n",
    "    eval_loss /= len(eval_loader.dataset)\n",
    "    print('====> Evaluation loss : {:.4f}'.format(eval_loss))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:1 [0/60000 (0.000000%)]\tLoss: 543.176797\n",
      "Train Epoch:1 [20000/60000 (33.333333%)]\tLoss: 188.354941\n",
      "Train Epoch:1 [40000/60000 (66.666667%)]\tLoss: 137.828613\n",
      "====> Epoch: 1 Average loss: 171.0613\n",
      "====> Evaluation loss : 129.7255\n",
      "Train Epoch:2 [0/60000 (0.000000%)]\tLoss: 131.830947\n",
      "Train Epoch:2 [20000/60000 (33.333333%)]\tLoss: 121.127588\n",
      "Train Epoch:2 [40000/60000 (66.666667%)]\tLoss: 113.783340\n",
      "====> Epoch: 2 Average loss: 117.7736\n",
      "====> Evaluation loss : 106.4475\n",
      "Train Epoch:3 [0/60000 (0.000000%)]\tLoss: 105.583223\n",
      "Train Epoch:3 [20000/60000 (33.333333%)]\tLoss: 102.671807\n",
      "Train Epoch:3 [40000/60000 (66.666667%)]\tLoss: 102.830605\n",
      "====> Epoch: 3 Average loss: 101.9621\n",
      "====> Evaluation loss : 96.4246\n",
      "Train Epoch:4 [0/60000 (0.000000%)]\tLoss: 98.518135\n",
      "Train Epoch:4 [20000/60000 (33.333333%)]\tLoss: 95.082109\n",
      "Train Epoch:4 [40000/60000 (66.666667%)]\tLoss: 91.735107\n",
      "====> Epoch: 4 Average loss: 93.9373\n",
      "====> Evaluation loss : 90.8421\n",
      "Train Epoch:5 [0/60000 (0.000000%)]\tLoss: 90.134033\n",
      "Train Epoch:5 [20000/60000 (33.333333%)]\tLoss: 94.369346\n",
      "Train Epoch:5 [40000/60000 (66.666667%)]\tLoss: 94.245762\n",
      "====> Epoch: 5 Average loss: 89.4157\n",
      "====> Evaluation loss : 86.8703\n",
      "Train Epoch:6 [0/60000 (0.000000%)]\tLoss: 87.092344\n",
      "Train Epoch:6 [20000/60000 (33.333333%)]\tLoss: 79.636797\n",
      "Train Epoch:6 [40000/60000 (66.666667%)]\tLoss: 85.593828\n",
      "====> Epoch: 6 Average loss: 86.2640\n",
      "====> Evaluation loss : 84.7581\n",
      "Train Epoch:7 [0/60000 (0.000000%)]\tLoss: 81.507446\n",
      "Train Epoch:7 [20000/60000 (33.333333%)]\tLoss: 83.120381\n",
      "Train Epoch:7 [40000/60000 (66.666667%)]\tLoss: 84.369639\n",
      "====> Epoch: 7 Average loss: 84.1200\n",
      "====> Evaluation loss : 82.9415\n",
      "Train Epoch:8 [0/60000 (0.000000%)]\tLoss: 78.490034\n",
      "Train Epoch:8 [20000/60000 (33.333333%)]\tLoss: 81.685688\n",
      "Train Epoch:8 [40000/60000 (66.666667%)]\tLoss: 77.458789\n",
      "====> Epoch: 8 Average loss: 82.4153\n",
      "====> Evaluation loss : 82.0957\n",
      "Train Epoch:9 [0/60000 (0.000000%)]\tLoss: 81.063853\n",
      "Train Epoch:9 [20000/60000 (33.333333%)]\tLoss: 82.857090\n",
      "Train Epoch:9 [40000/60000 (66.666667%)]\tLoss: 80.872036\n",
      "====> Epoch: 9 Average loss: 81.0383\n",
      "====> Evaluation loss : 80.9214\n",
      "Train Epoch:10 [0/60000 (0.000000%)]\tLoss: 78.350698\n",
      "Train Epoch:10 [20000/60000 (33.333333%)]\tLoss: 84.125420\n",
      "Train Epoch:10 [40000/60000 (66.666667%)]\tLoss: 81.210479\n",
      "====> Epoch: 10 Average loss: 79.9298\n",
      "====> Evaluation loss : 79.4604\n",
      "Train Epoch:11 [0/60000 (0.000000%)]\tLoss: 76.291636\n",
      "Train Epoch:11 [20000/60000 (33.333333%)]\tLoss: 76.065771\n",
      "Train Epoch:11 [40000/60000 (66.666667%)]\tLoss: 78.923784\n",
      "====> Epoch: 11 Average loss: 78.9423\n",
      "====> Evaluation loss : 78.8956\n",
      "Train Epoch:12 [0/60000 (0.000000%)]\tLoss: 79.933516\n",
      "Train Epoch:12 [20000/60000 (33.333333%)]\tLoss: 79.695312\n",
      "Train Epoch:12 [40000/60000 (66.666667%)]\tLoss: 77.896611\n",
      "====> Epoch: 12 Average loss: 78.1631\n",
      "====> Evaluation loss : 78.4297\n",
      "Train Epoch:13 [0/60000 (0.000000%)]\tLoss: 77.228931\n",
      "Train Epoch:13 [20000/60000 (33.333333%)]\tLoss: 76.429736\n",
      "Train Epoch:13 [40000/60000 (66.666667%)]\tLoss: 81.122036\n",
      "====> Epoch: 13 Average loss: 77.4255\n",
      "====> Evaluation loss : 77.7390\n",
      "Train Epoch:14 [0/60000 (0.000000%)]\tLoss: 75.601641\n",
      "Train Epoch:14 [20000/60000 (33.333333%)]\tLoss: 72.749687\n",
      "Train Epoch:14 [40000/60000 (66.666667%)]\tLoss: 83.627588\n",
      "====> Epoch: 14 Average loss: 76.8149\n",
      "====> Evaluation loss : 76.8403\n",
      "Train Epoch:15 [0/60000 (0.000000%)]\tLoss: 74.716074\n",
      "Train Epoch:15 [20000/60000 (33.333333%)]\tLoss: 77.008057\n",
      "Train Epoch:15 [40000/60000 (66.666667%)]\tLoss: 81.470288\n",
      "====> Epoch: 15 Average loss: 76.2424\n",
      "====> Evaluation loss : 76.7509\n",
      "Train Epoch:16 [0/60000 (0.000000%)]\tLoss: 79.100176\n",
      "Train Epoch:16 [20000/60000 (33.333333%)]\tLoss: 79.442397\n",
      "Train Epoch:16 [40000/60000 (66.666667%)]\tLoss: 74.620254\n",
      "====> Epoch: 16 Average loss: 75.7912\n",
      "====> Evaluation loss : 76.6341\n",
      "Train Epoch:17 [0/60000 (0.000000%)]\tLoss: 78.529395\n",
      "Train Epoch:17 [20000/60000 (33.333333%)]\tLoss: 73.314893\n",
      "Train Epoch:17 [40000/60000 (66.666667%)]\tLoss: 81.571460\n",
      "====> Epoch: 17 Average loss: 75.3274\n",
      "====> Evaluation loss : 76.1561\n",
      "Train Epoch:18 [0/60000 (0.000000%)]\tLoss: 74.285132\n",
      "Train Epoch:18 [20000/60000 (33.333333%)]\tLoss: 73.047217\n",
      "Train Epoch:18 [40000/60000 (66.666667%)]\tLoss: 74.989121\n",
      "====> Epoch: 18 Average loss: 74.8110\n",
      "====> Evaluation loss : 75.8014\n",
      "Train Epoch:19 [0/60000 (0.000000%)]\tLoss: 70.908501\n",
      "Train Epoch:19 [20000/60000 (33.333333%)]\tLoss: 74.354658\n",
      "Train Epoch:19 [40000/60000 (66.666667%)]\tLoss: 75.346035\n",
      "====> Epoch: 19 Average loss: 74.4697\n",
      "====> Evaluation loss : 75.3300\n",
      "Train Epoch:20 [0/60000 (0.000000%)]\tLoss: 77.197935\n",
      "Train Epoch:20 [20000/60000 (33.333333%)]\tLoss: 69.969111\n",
      "Train Epoch:20 [40000/60000 (66.666667%)]\tLoss: 70.068560\n",
      "====> Epoch: 20 Average loss: 74.1354\n",
      "====> Evaluation loss : 75.2265\n",
      "Train Epoch:21 [0/60000 (0.000000%)]\tLoss: 72.575459\n",
      "Train Epoch:21 [20000/60000 (33.333333%)]\tLoss: 72.440928\n",
      "Train Epoch:21 [40000/60000 (66.666667%)]\tLoss: 74.089541\n",
      "====> Epoch: 21 Average loss: 73.7874\n",
      "====> Evaluation loss : 74.9982\n",
      "Train Epoch:22 [0/60000 (0.000000%)]\tLoss: 76.407988\n",
      "Train Epoch:22 [20000/60000 (33.333333%)]\tLoss: 68.367485\n",
      "Train Epoch:22 [40000/60000 (66.666667%)]\tLoss: 72.884424\n",
      "====> Epoch: 22 Average loss: 73.5327\n",
      "====> Evaluation loss : 75.1538\n",
      "Train Epoch:23 [0/60000 (0.000000%)]\tLoss: 70.590146\n",
      "Train Epoch:23 [20000/60000 (33.333333%)]\tLoss: 71.961323\n",
      "Train Epoch:23 [40000/60000 (66.666667%)]\tLoss: 72.220791\n",
      "====> Epoch: 23 Average loss: 73.1700\n",
      "====> Evaluation loss : 74.6245\n",
      "Train Epoch:24 [0/60000 (0.000000%)]\tLoss: 72.917695\n",
      "Train Epoch:24 [20000/60000 (33.333333%)]\tLoss: 70.595747\n",
      "Train Epoch:24 [40000/60000 (66.666667%)]\tLoss: 71.851392\n",
      "====> Epoch: 24 Average loss: 72.9380\n",
      "====> Evaluation loss : 74.3421\n",
      "Train Epoch:25 [0/60000 (0.000000%)]\tLoss: 73.884795\n",
      "Train Epoch:25 [20000/60000 (33.333333%)]\tLoss: 74.486143\n",
      "Train Epoch:25 [40000/60000 (66.666667%)]\tLoss: 69.834810\n",
      "====> Epoch: 25 Average loss: 72.7306\n",
      "====> Evaluation loss : 74.4391\n",
      "Train Epoch:26 [0/60000 (0.000000%)]\tLoss: 73.683931\n",
      "Train Epoch:26 [20000/60000 (33.333333%)]\tLoss: 74.449585\n",
      "Train Epoch:26 [40000/60000 (66.666667%)]\tLoss: 73.136479\n",
      "====> Epoch: 26 Average loss: 72.4661\n",
      "====> Evaluation loss : 74.1293\n",
      "Train Epoch:27 [0/60000 (0.000000%)]\tLoss: 73.451055\n",
      "Train Epoch:27 [20000/60000 (33.333333%)]\tLoss: 75.813599\n",
      "Train Epoch:27 [40000/60000 (66.666667%)]\tLoss: 76.057324\n",
      "====> Epoch: 27 Average loss: 72.2454\n",
      "====> Evaluation loss : 73.9738\n",
      "Train Epoch:28 [0/60000 (0.000000%)]\tLoss: 72.944585\n",
      "Train Epoch:28 [20000/60000 (33.333333%)]\tLoss: 70.567510\n",
      "Train Epoch:28 [40000/60000 (66.666667%)]\tLoss: 74.294829\n",
      "====> Epoch: 28 Average loss: 72.0440\n",
      "====> Evaluation loss : 73.9680\n",
      "Train Epoch:29 [0/60000 (0.000000%)]\tLoss: 68.264751\n",
      "Train Epoch:29 [20000/60000 (33.333333%)]\tLoss: 71.616597\n",
      "Train Epoch:29 [40000/60000 (66.666667%)]\tLoss: 72.127212\n",
      "====> Epoch: 29 Average loss: 71.8320\n",
      "====> Evaluation loss : 73.6075\n",
      "Train Epoch:30 [0/60000 (0.000000%)]\tLoss: 72.837280\n",
      "Train Epoch:30 [20000/60000 (33.333333%)]\tLoss: 72.559160\n",
      "Train Epoch:30 [40000/60000 (66.666667%)]\tLoss: 70.527261\n",
      "====> Epoch: 30 Average loss: 71.6628\n",
      "====> Evaluation loss : 73.6890\n",
      "Train Epoch:31 [0/60000 (0.000000%)]\tLoss: 69.165239\n",
      "Train Epoch:31 [20000/60000 (33.333333%)]\tLoss: 71.061201\n",
      "Train Epoch:31 [40000/60000 (66.666667%)]\tLoss: 77.144941\n",
      "====> Epoch: 31 Average loss: 71.4570\n",
      "====> Evaluation loss : 73.4387\n",
      "Train Epoch:32 [0/60000 (0.000000%)]\tLoss: 71.179673\n",
      "Train Epoch:32 [20000/60000 (33.333333%)]\tLoss: 74.424165\n",
      "Train Epoch:32 [40000/60000 (66.666667%)]\tLoss: 71.739839\n",
      "====> Epoch: 32 Average loss: 71.2894\n",
      "====> Evaluation loss : 73.3725\n",
      "Train Epoch:33 [0/60000 (0.000000%)]\tLoss: 71.021748\n",
      "Train Epoch:33 [20000/60000 (33.333333%)]\tLoss: 70.542354\n",
      "Train Epoch:33 [40000/60000 (66.666667%)]\tLoss: 69.191597\n",
      "====> Epoch: 33 Average loss: 71.1257\n",
      "====> Evaluation loss : 73.4798\n",
      "Train Epoch:34 [0/60000 (0.000000%)]\tLoss: 67.473340\n",
      "Train Epoch:34 [20000/60000 (33.333333%)]\tLoss: 69.991743\n",
      "Train Epoch:34 [40000/60000 (66.666667%)]\tLoss: 70.792988\n",
      "====> Epoch: 34 Average loss: 70.9898\n",
      "====> Evaluation loss : 73.2741\n",
      "Train Epoch:35 [0/60000 (0.000000%)]\tLoss: 68.623291\n",
      "Train Epoch:35 [20000/60000 (33.333333%)]\tLoss: 71.384961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:35 [40000/60000 (66.666667%)]\tLoss: 70.900469\n",
      "====> Epoch: 35 Average loss: 70.8938\n",
      "====> Evaluation loss : 73.3783\n",
      "Train Epoch:36 [0/60000 (0.000000%)]\tLoss: 69.936411\n",
      "Train Epoch:36 [20000/60000 (33.333333%)]\tLoss: 69.090049\n",
      "Train Epoch:36 [40000/60000 (66.666667%)]\tLoss: 71.723970\n",
      "====> Epoch: 36 Average loss: 70.7463\n",
      "====> Evaluation loss : 73.2114\n",
      "Train Epoch:37 [0/60000 (0.000000%)]\tLoss: 71.189121\n",
      "Train Epoch:37 [20000/60000 (33.333333%)]\tLoss: 69.826602\n",
      "Train Epoch:37 [40000/60000 (66.666667%)]\tLoss: 71.067861\n",
      "====> Epoch: 37 Average loss: 70.6186\n",
      "====> Evaluation loss : 73.1550\n",
      "Train Epoch:38 [0/60000 (0.000000%)]\tLoss: 71.140283\n",
      "Train Epoch:38 [20000/60000 (33.333333%)]\tLoss: 68.852041\n",
      "Train Epoch:38 [40000/60000 (66.666667%)]\tLoss: 69.497930\n",
      "====> Epoch: 38 Average loss: 70.4653\n",
      "====> Evaluation loss : 73.2979\n",
      "Train Epoch:39 [0/60000 (0.000000%)]\tLoss: 69.765317\n",
      "Train Epoch:39 [20000/60000 (33.333333%)]\tLoss: 69.619277\n",
      "Train Epoch:39 [40000/60000 (66.666667%)]\tLoss: 72.266235\n",
      "====> Epoch: 39 Average loss: 70.3539\n",
      "====> Evaluation loss : 72.9600\n",
      "Train Epoch:40 [0/60000 (0.000000%)]\tLoss: 69.583716\n",
      "Train Epoch:40 [20000/60000 (33.333333%)]\tLoss: 71.554683\n",
      "Train Epoch:40 [40000/60000 (66.666667%)]\tLoss: 69.563418\n",
      "====> Epoch: 40 Average loss: 70.2164\n",
      "====> Evaluation loss : 72.9778\n",
      "Train Epoch:41 [0/60000 (0.000000%)]\tLoss: 70.817852\n",
      "Train Epoch:41 [20000/60000 (33.333333%)]\tLoss: 70.240352\n",
      "Train Epoch:41 [40000/60000 (66.666667%)]\tLoss: 69.266206\n",
      "====> Epoch: 41 Average loss: 70.1154\n",
      "====> Evaluation loss : 72.8712\n",
      "Train Epoch:42 [0/60000 (0.000000%)]\tLoss: 69.228418\n",
      "Train Epoch:42 [20000/60000 (33.333333%)]\tLoss: 68.259424\n",
      "Train Epoch:42 [40000/60000 (66.666667%)]\tLoss: 66.914390\n",
      "====> Epoch: 42 Average loss: 69.9755\n",
      "====> Evaluation loss : 72.7372\n",
      "Train Epoch:43 [0/60000 (0.000000%)]\tLoss: 71.657690\n",
      "Train Epoch:43 [20000/60000 (33.333333%)]\tLoss: 70.348789\n",
      "Train Epoch:43 [40000/60000 (66.666667%)]\tLoss: 71.120801\n",
      "====> Epoch: 43 Average loss: 69.8716\n",
      "====> Evaluation loss : 73.0467\n",
      "Train Epoch:44 [0/60000 (0.000000%)]\tLoss: 69.101069\n",
      "Train Epoch:44 [20000/60000 (33.333333%)]\tLoss: 70.447285\n",
      "Train Epoch:44 [40000/60000 (66.666667%)]\tLoss: 68.455732\n",
      "====> Epoch: 44 Average loss: 69.8783\n",
      "====> Evaluation loss : 72.7722\n",
      "Train Epoch:45 [0/60000 (0.000000%)]\tLoss: 64.551382\n",
      "Train Epoch:45 [20000/60000 (33.333333%)]\tLoss: 68.440337\n",
      "Train Epoch:45 [40000/60000 (66.666667%)]\tLoss: 66.141104\n",
      "====> Epoch: 45 Average loss: 69.7321\n",
      "====> Evaluation loss : 72.6770\n",
      "Train Epoch:46 [0/60000 (0.000000%)]\tLoss: 66.505850\n",
      "Train Epoch:46 [20000/60000 (33.333333%)]\tLoss: 66.618862\n",
      "Train Epoch:46 [40000/60000 (66.666667%)]\tLoss: 71.038457\n",
      "====> Epoch: 46 Average loss: 69.6513\n",
      "====> Evaluation loss : 72.7095\n",
      "Train Epoch:47 [0/60000 (0.000000%)]\tLoss: 67.706108\n",
      "Train Epoch:47 [20000/60000 (33.333333%)]\tLoss: 69.390176\n",
      "Train Epoch:47 [40000/60000 (66.666667%)]\tLoss: 70.227871\n",
      "====> Epoch: 47 Average loss: 69.5431\n",
      "====> Evaluation loss : 72.5913\n",
      "Train Epoch:48 [0/60000 (0.000000%)]\tLoss: 71.624097\n",
      "Train Epoch:48 [20000/60000 (33.333333%)]\tLoss: 71.570728\n",
      "Train Epoch:48 [40000/60000 (66.666667%)]\tLoss: 67.308931\n",
      "====> Epoch: 48 Average loss: 69.4323\n",
      "====> Evaluation loss : 72.6106\n",
      "Train Epoch:49 [0/60000 (0.000000%)]\tLoss: 70.701299\n",
      "Train Epoch:49 [20000/60000 (33.333333%)]\tLoss: 66.252134\n",
      "Train Epoch:49 [40000/60000 (66.666667%)]\tLoss: 69.615737\n",
      "====> Epoch: 49 Average loss: 69.3839\n",
      "====> Evaluation loss : 72.5907\n",
      "Train Epoch:50 [0/60000 (0.000000%)]\tLoss: 67.413950\n",
      "Train Epoch:50 [20000/60000 (33.333333%)]\tLoss: 69.283452\n",
      "Train Epoch:50 [40000/60000 (66.666667%)]\tLoss: 70.897524\n",
      "====> Epoch: 50 Average loss: 69.2603\n",
      "====> Evaluation loss : 72.5317\n",
      "Train Epoch:51 [0/60000 (0.000000%)]\tLoss: 70.531045\n",
      "Train Epoch:51 [20000/60000 (33.333333%)]\tLoss: 67.890991\n",
      "Train Epoch:51 [40000/60000 (66.666667%)]\tLoss: 68.111733\n",
      "====> Epoch: 51 Average loss: 69.1827\n",
      "====> Evaluation loss : 72.6206\n",
      "Train Epoch:52 [0/60000 (0.000000%)]\tLoss: 68.216997\n",
      "Train Epoch:52 [20000/60000 (33.333333%)]\tLoss: 73.635864\n",
      "Train Epoch:52 [40000/60000 (66.666667%)]\tLoss: 68.779712\n",
      "====> Epoch: 52 Average loss: 69.1172\n",
      "====> Evaluation loss : 72.4684\n",
      "Train Epoch:53 [0/60000 (0.000000%)]\tLoss: 67.659189\n",
      "Train Epoch:53 [20000/60000 (33.333333%)]\tLoss: 66.731646\n",
      "Train Epoch:53 [40000/60000 (66.666667%)]\tLoss: 69.877397\n",
      "====> Epoch: 53 Average loss: 69.0415\n",
      "====> Evaluation loss : 72.4803\n",
      "Train Epoch:54 [0/60000 (0.000000%)]\tLoss: 69.342759\n",
      "Train Epoch:54 [20000/60000 (33.333333%)]\tLoss: 69.621416\n",
      "Train Epoch:54 [40000/60000 (66.666667%)]\tLoss: 70.396348\n",
      "====> Epoch: 54 Average loss: 69.0003\n",
      "====> Evaluation loss : 72.3682\n",
      "Train Epoch:55 [0/60000 (0.000000%)]\tLoss: 64.718276\n",
      "Train Epoch:55 [20000/60000 (33.333333%)]\tLoss: 69.717803\n",
      "Train Epoch:55 [40000/60000 (66.666667%)]\tLoss: 70.267241\n",
      "====> Epoch: 55 Average loss: 68.9430\n",
      "====> Evaluation loss : 72.3446\n",
      "Train Epoch:56 [0/60000 (0.000000%)]\tLoss: 67.562441\n",
      "Train Epoch:56 [20000/60000 (33.333333%)]\tLoss: 68.300171\n",
      "Train Epoch:56 [40000/60000 (66.666667%)]\tLoss: 69.479961\n",
      "====> Epoch: 56 Average loss: 68.8569\n",
      "====> Evaluation loss : 72.4285\n",
      "Train Epoch:57 [0/60000 (0.000000%)]\tLoss: 71.329463\n",
      "Train Epoch:57 [20000/60000 (33.333333%)]\tLoss: 67.827881\n",
      "Train Epoch:57 [40000/60000 (66.666667%)]\tLoss: 67.946030\n",
      "====> Epoch: 57 Average loss: 68.7860\n",
      "====> Evaluation loss : 72.4170\n",
      "Train Epoch:58 [0/60000 (0.000000%)]\tLoss: 67.170063\n",
      "Train Epoch:58 [20000/60000 (33.333333%)]\tLoss: 68.565854\n",
      "Train Epoch:58 [40000/60000 (66.666667%)]\tLoss: 67.580933\n",
      "====> Epoch: 58 Average loss: 68.7061\n",
      "====> Evaluation loss : 72.5123\n",
      "Train Epoch:59 [0/60000 (0.000000%)]\tLoss: 70.915801\n",
      "Train Epoch:59 [20000/60000 (33.333333%)]\tLoss: 67.944004\n",
      "Train Epoch:59 [40000/60000 (66.666667%)]\tLoss: 67.459917\n",
      "====> Epoch: 59 Average loss: 68.6394\n",
      "====> Evaluation loss : 72.3399\n",
      "Train Epoch:60 [0/60000 (0.000000%)]\tLoss: 66.596099\n",
      "Train Epoch:60 [20000/60000 (33.333333%)]\tLoss: 67.008066\n",
      "Train Epoch:60 [40000/60000 (66.666667%)]\tLoss: 70.825293\n",
      "====> Epoch: 60 Average loss: 68.5651\n",
      "====> Evaluation loss : 72.2418\n",
      "Train Epoch:61 [0/60000 (0.000000%)]\tLoss: 66.056724\n",
      "Train Epoch:61 [20000/60000 (33.333333%)]\tLoss: 66.621621\n",
      "Train Epoch:61 [40000/60000 (66.666667%)]\tLoss: 68.235825\n",
      "====> Epoch: 61 Average loss: 68.5707\n",
      "====> Evaluation loss : 72.4065\n",
      "Train Epoch:62 [0/60000 (0.000000%)]\tLoss: 67.435186\n",
      "Train Epoch:62 [20000/60000 (33.333333%)]\tLoss: 69.868208\n",
      "Train Epoch:62 [40000/60000 (66.666667%)]\tLoss: 69.397324\n",
      "====> Epoch: 62 Average loss: 68.4526\n",
      "====> Evaluation loss : 72.3474\n",
      "Train Epoch:63 [0/60000 (0.000000%)]\tLoss: 67.723799\n",
      "Train Epoch:63 [20000/60000 (33.333333%)]\tLoss: 68.541494\n",
      "Train Epoch:63 [40000/60000 (66.666667%)]\tLoss: 67.101455\n",
      "====> Epoch: 63 Average loss: 68.4193\n",
      "====> Evaluation loss : 72.2594\n",
      "Train Epoch:64 [0/60000 (0.000000%)]\tLoss: 69.405923\n",
      "Train Epoch:64 [20000/60000 (33.333333%)]\tLoss: 72.388994\n",
      "Train Epoch:64 [40000/60000 (66.666667%)]\tLoss: 67.388325\n",
      "====> Epoch: 64 Average loss: 68.3683\n",
      "====> Evaluation loss : 72.4250\n",
      "Train Epoch:65 [0/60000 (0.000000%)]\tLoss: 68.614683\n",
      "Train Epoch:65 [20000/60000 (33.333333%)]\tLoss: 65.264482\n",
      "Train Epoch:65 [40000/60000 (66.666667%)]\tLoss: 70.641025\n",
      "====> Epoch: 65 Average loss: 68.2824\n",
      "====> Evaluation loss : 72.3640\n",
      "Train Epoch:66 [0/60000 (0.000000%)]\tLoss: 67.510591\n",
      "Train Epoch:66 [20000/60000 (33.333333%)]\tLoss: 67.507090\n",
      "Train Epoch:66 [40000/60000 (66.666667%)]\tLoss: 69.390601\n",
      "====> Epoch: 66 Average loss: 68.2773\n",
      "====> Evaluation loss : 72.4013\n",
      "Train Epoch:67 [0/60000 (0.000000%)]\tLoss: 67.859189\n",
      "Train Epoch:67 [20000/60000 (33.333333%)]\tLoss: 67.445552\n",
      "Train Epoch:67 [40000/60000 (66.666667%)]\tLoss: 70.019116\n",
      "====> Epoch: 67 Average loss: 68.1849\n",
      "====> Evaluation loss : 72.4348\n",
      "Train Epoch:68 [0/60000 (0.000000%)]\tLoss: 68.030776\n",
      "Train Epoch:68 [20000/60000 (33.333333%)]\tLoss: 67.979810\n",
      "Train Epoch:68 [40000/60000 (66.666667%)]\tLoss: 68.594946\n",
      "====> Epoch: 68 Average loss: 68.1085\n",
      "====> Evaluation loss : 72.1224\n",
      "Train Epoch:69 [0/60000 (0.000000%)]\tLoss: 67.466045\n",
      "Train Epoch:69 [20000/60000 (33.333333%)]\tLoss: 66.428232\n",
      "Train Epoch:69 [40000/60000 (66.666667%)]\tLoss: 70.673535\n",
      "====> Epoch: 69 Average loss: 68.1310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Evaluation loss : 72.2393\n",
      "Train Epoch:70 [0/60000 (0.000000%)]\tLoss: 65.322236\n",
      "Train Epoch:70 [20000/60000 (33.333333%)]\tLoss: 67.719258\n",
      "Train Epoch:70 [40000/60000 (66.666667%)]\tLoss: 68.655732\n",
      "====> Epoch: 70 Average loss: 68.0661\n",
      "====> Evaluation loss : 72.1938\n",
      "Train Epoch:71 [0/60000 (0.000000%)]\tLoss: 66.256675\n",
      "Train Epoch:71 [20000/60000 (33.333333%)]\tLoss: 66.727036\n",
      "Train Epoch:71 [40000/60000 (66.666667%)]\tLoss: 66.001016\n",
      "====> Epoch: 71 Average loss: 68.0201\n",
      "====> Evaluation loss : 72.2430\n",
      "Train Epoch:72 [0/60000 (0.000000%)]\tLoss: 66.535488\n",
      "Train Epoch:72 [20000/60000 (33.333333%)]\tLoss: 68.852700\n",
      "Train Epoch:72 [40000/60000 (66.666667%)]\tLoss: 67.609019\n",
      "====> Epoch: 72 Average loss: 67.9541\n",
      "====> Evaluation loss : 72.3576\n",
      "Train Epoch:73 [0/60000 (0.000000%)]\tLoss: 71.980352\n",
      "Train Epoch:73 [20000/60000 (33.333333%)]\tLoss: 67.841470\n",
      "Train Epoch:73 [40000/60000 (66.666667%)]\tLoss: 68.304189\n",
      "====> Epoch: 73 Average loss: 67.9022\n",
      "====> Evaluation loss : 72.2727\n",
      "Train Epoch:74 [0/60000 (0.000000%)]\tLoss: 66.778804\n",
      "Train Epoch:74 [20000/60000 (33.333333%)]\tLoss: 63.510044\n",
      "Train Epoch:74 [40000/60000 (66.666667%)]\tLoss: 70.172778\n",
      "====> Epoch: 74 Average loss: 67.8803\n",
      "====> Evaluation loss : 72.2026\n",
      "Train Epoch:75 [0/60000 (0.000000%)]\tLoss: 70.567988\n",
      "Train Epoch:75 [20000/60000 (33.333333%)]\tLoss: 68.324043\n",
      "Train Epoch:75 [40000/60000 (66.666667%)]\tLoss: 66.968892\n",
      "====> Epoch: 75 Average loss: 67.8533\n",
      "====> Evaluation loss : 72.2098\n",
      "Train Epoch:76 [0/60000 (0.000000%)]\tLoss: 67.856455\n",
      "Train Epoch:76 [20000/60000 (33.333333%)]\tLoss: 63.214072\n",
      "Train Epoch:76 [40000/60000 (66.666667%)]\tLoss: 69.526079\n",
      "====> Epoch: 76 Average loss: 67.7852\n",
      "====> Evaluation loss : 72.4217\n",
      "Train Epoch:77 [0/60000 (0.000000%)]\tLoss: 67.210952\n",
      "Train Epoch:77 [20000/60000 (33.333333%)]\tLoss: 67.413784\n",
      "Train Epoch:77 [40000/60000 (66.666667%)]\tLoss: 65.195947\n",
      "====> Epoch: 77 Average loss: 67.7538\n",
      "====> Evaluation loss : 72.3296\n",
      "Train Epoch:78 [0/60000 (0.000000%)]\tLoss: 66.683926\n",
      "Train Epoch:78 [20000/60000 (33.333333%)]\tLoss: 66.242012\n",
      "Train Epoch:78 [40000/60000 (66.666667%)]\tLoss: 68.400684\n",
      "====> Epoch: 78 Average loss: 67.6881\n",
      "====> Evaluation loss : 72.2687\n",
      "Train Epoch:79 [0/60000 (0.000000%)]\tLoss: 71.086655\n",
      "Train Epoch:79 [20000/60000 (33.333333%)]\tLoss: 68.256294\n",
      "Train Epoch:79 [40000/60000 (66.666667%)]\tLoss: 69.142534\n",
      "====> Epoch: 79 Average loss: 67.7069\n",
      "====> Evaluation loss : 72.1303\n",
      "Train Epoch:80 [0/60000 (0.000000%)]\tLoss: 68.524678\n",
      "Train Epoch:80 [20000/60000 (33.333333%)]\tLoss: 64.582690\n",
      "Train Epoch:80 [40000/60000 (66.666667%)]\tLoss: 66.314224\n",
      "====> Epoch: 80 Average loss: 67.6442\n",
      "====> Evaluation loss : 72.0282\n",
      "Train Epoch:81 [0/60000 (0.000000%)]\tLoss: 65.731753\n",
      "Train Epoch:81 [20000/60000 (33.333333%)]\tLoss: 71.354292\n",
      "Train Epoch:81 [40000/60000 (66.666667%)]\tLoss: 69.870498\n",
      "====> Epoch: 81 Average loss: 67.6399\n",
      "====> Evaluation loss : 72.3879\n",
      "Train Epoch:82 [0/60000 (0.000000%)]\tLoss: 66.274434\n",
      "Train Epoch:82 [20000/60000 (33.333333%)]\tLoss: 69.026787\n",
      "Train Epoch:82 [40000/60000 (66.666667%)]\tLoss: 69.359580\n",
      "====> Epoch: 82 Average loss: 67.6104\n",
      "====> Evaluation loss : 72.2246\n",
      "Train Epoch:83 [0/60000 (0.000000%)]\tLoss: 68.436348\n",
      "Train Epoch:83 [20000/60000 (33.333333%)]\tLoss: 70.139780\n",
      "Train Epoch:83 [40000/60000 (66.666667%)]\tLoss: 66.018892\n",
      "====> Epoch: 83 Average loss: 67.5292\n",
      "====> Evaluation loss : 72.1583\n",
      "Train Epoch:84 [0/60000 (0.000000%)]\tLoss: 65.784321\n",
      "Train Epoch:84 [20000/60000 (33.333333%)]\tLoss: 66.510845\n",
      "Train Epoch:84 [40000/60000 (66.666667%)]\tLoss: 64.797676\n",
      "====> Epoch: 84 Average loss: 67.5100\n",
      "====> Evaluation loss : 72.2555\n",
      "Train Epoch:85 [0/60000 (0.000000%)]\tLoss: 66.871392\n",
      "Train Epoch:85 [20000/60000 (33.333333%)]\tLoss: 66.165439\n",
      "Train Epoch:85 [40000/60000 (66.666667%)]\tLoss: 67.052148\n",
      "====> Epoch: 85 Average loss: 67.4682\n",
      "====> Evaluation loss : 72.0867\n",
      "Train Epoch:86 [0/60000 (0.000000%)]\tLoss: 66.957402\n",
      "Train Epoch:86 [20000/60000 (33.333333%)]\tLoss: 69.285835\n",
      "Train Epoch:86 [40000/60000 (66.666667%)]\tLoss: 65.285425\n",
      "====> Epoch: 86 Average loss: 67.4238\n",
      "====> Evaluation loss : 72.0569\n",
      "Train Epoch:87 [0/60000 (0.000000%)]\tLoss: 70.687739\n",
      "Train Epoch:87 [20000/60000 (33.333333%)]\tLoss: 67.432090\n",
      "Train Epoch:87 [40000/60000 (66.666667%)]\tLoss: 67.456133\n",
      "====> Epoch: 87 Average loss: 67.3493\n",
      "====> Evaluation loss : 72.2250\n",
      "Train Epoch:88 [0/60000 (0.000000%)]\tLoss: 65.862480\n",
      "Train Epoch:88 [20000/60000 (33.333333%)]\tLoss: 68.080190\n",
      "Train Epoch:88 [40000/60000 (66.666667%)]\tLoss: 68.842266\n",
      "====> Epoch: 88 Average loss: 67.3777\n",
      "====> Evaluation loss : 72.2304\n",
      "Train Epoch:89 [0/60000 (0.000000%)]\tLoss: 67.761870\n",
      "Train Epoch:89 [20000/60000 (33.333333%)]\tLoss: 66.164683\n",
      "Train Epoch:89 [40000/60000 (66.666667%)]\tLoss: 69.165791\n",
      "====> Epoch: 89 Average loss: 67.2868\n",
      "====> Evaluation loss : 72.2956\n",
      "Train Epoch:90 [0/60000 (0.000000%)]\tLoss: 68.272520\n",
      "Train Epoch:90 [20000/60000 (33.333333%)]\tLoss: 69.014922\n",
      "Train Epoch:90 [40000/60000 (66.666667%)]\tLoss: 65.972583\n",
      "====> Epoch: 90 Average loss: 67.2870\n",
      "====> Evaluation loss : 72.3841\n",
      "Train Epoch:91 [0/60000 (0.000000%)]\tLoss: 66.176772\n",
      "Train Epoch:91 [20000/60000 (33.333333%)]\tLoss: 66.207778\n",
      "Train Epoch:91 [40000/60000 (66.666667%)]\tLoss: 71.277988\n",
      "====> Epoch: 91 Average loss: 67.2742\n",
      "====> Evaluation loss : 72.0808\n",
      "Train Epoch:92 [0/60000 (0.000000%)]\tLoss: 68.844268\n",
      "Train Epoch:92 [20000/60000 (33.333333%)]\tLoss: 66.678101\n",
      "Train Epoch:92 [40000/60000 (66.666667%)]\tLoss: 66.407476\n",
      "====> Epoch: 92 Average loss: 67.2636\n",
      "====> Evaluation loss : 72.2949\n",
      "Train Epoch:93 [0/60000 (0.000000%)]\tLoss: 67.811187\n",
      "Train Epoch:93 [20000/60000 (33.333333%)]\tLoss: 66.745015\n",
      "Train Epoch:93 [40000/60000 (66.666667%)]\tLoss: 70.127090\n",
      "====> Epoch: 93 Average loss: 67.1994\n",
      "====> Evaluation loss : 72.2545\n",
      "Train Epoch:94 [0/60000 (0.000000%)]\tLoss: 68.047393\n",
      "Train Epoch:94 [20000/60000 (33.333333%)]\tLoss: 69.413052\n",
      "Train Epoch:94 [40000/60000 (66.666667%)]\tLoss: 66.812227\n",
      "====> Epoch: 94 Average loss: 67.1682\n",
      "====> Evaluation loss : 72.3290\n",
      "Train Epoch:95 [0/60000 (0.000000%)]\tLoss: 67.488306\n",
      "Train Epoch:95 [20000/60000 (33.333333%)]\tLoss: 64.947588\n",
      "Train Epoch:95 [40000/60000 (66.666667%)]\tLoss: 66.226992\n",
      "====> Epoch: 95 Average loss: 67.0924\n",
      "====> Evaluation loss : 72.2277\n",
      "Train Epoch:96 [0/60000 (0.000000%)]\tLoss: 68.020938\n",
      "Train Epoch:96 [20000/60000 (33.333333%)]\tLoss: 66.677129\n",
      "Train Epoch:96 [40000/60000 (66.666667%)]\tLoss: 67.433862\n",
      "====> Epoch: 96 Average loss: 67.1300\n",
      "====> Evaluation loss : 72.3051\n",
      "Train Epoch:97 [0/60000 (0.000000%)]\tLoss: 69.818999\n",
      "Train Epoch:97 [20000/60000 (33.333333%)]\tLoss: 68.418442\n",
      "Train Epoch:97 [40000/60000 (66.666667%)]\tLoss: 67.197959\n",
      "====> Epoch: 97 Average loss: 67.0713\n",
      "====> Evaluation loss : 72.2848\n",
      "Train Epoch:98 [0/60000 (0.000000%)]\tLoss: 70.370942\n",
      "Train Epoch:98 [20000/60000 (33.333333%)]\tLoss: 64.021499\n",
      "Train Epoch:98 [40000/60000 (66.666667%)]\tLoss: 71.209424\n",
      "====> Epoch: 98 Average loss: 67.0796\n",
      "====> Evaluation loss : 72.4357\n",
      "Train Epoch:99 [0/60000 (0.000000%)]\tLoss: 68.015142\n",
      "Train Epoch:99 [20000/60000 (33.333333%)]\tLoss: 66.890337\n",
      "Train Epoch:99 [40000/60000 (66.666667%)]\tLoss: 66.285391\n",
      "====> Epoch: 99 Average loss: 67.0213\n",
      "====> Evaluation loss : 72.1098\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 100):\n",
    "    train(epoch)\n",
    "    evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[100, 1, 28, 28]' is invalid for input of size 6272",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-53befbd858d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mrecon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEMPERATURE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         comparison = torch.cat([data[:n].view(BATCH_SIZE, 1, 28, 28)[:n],\n\u001b[0m\u001b[1;32m      9\u001b[0m                                     recon_batch.view(BATCH_SIZE, 1, 28, 28)[:n]])\n\u001b[1;32m     10\u001b[0m         save_image(comparison.data.cpu(),\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[100, 1, 28, 28]' is invalid for input of size 6272"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for batch_ind, (data, _) in enumerate(eval_loader):\n",
    "#             data = data.cuda()\n",
    "        \n",
    "        data = data.view(BATCH_SIZE, -1)\n",
    "        recon, qy = vae(data, TEMPERATURE)\n",
    "        n = min(data.size(0), 8)\n",
    "        comparison = torch.cat([data[:n].view(BATCH_SIZE, 28, 28)[:n],\n",
    "                                    recon_batch.view(BATCH_SIZE, 1, 28, 28)[:n]])\n",
    "        save_image(comparison.data.cpu(),\n",
    "                       'samples/sample_gumbel_softmax' +'.png', nrow=n)\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
